{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnanyaGupta24/PortfolioOptimization/blob/main/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, this code sets up an authenticated connection to Google Drive API that allows the user to access and manage files stored in their Google Drive account programmatically."
      ],
      "metadata": {
        "id": "g-4_XAAVgAmr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ0CVfKVe99j"
      },
      "outputs": [],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code provides a simple way to download a file from Google Drive into a Google Colab environment, which can then be used for data analysis, machine learning, or any other purposes."
      ],
      "metadata": {
        "id": "wU9IvXpXgZ6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This part linkes the dataset in the drive to the google colab file\n",
        "link = 'link here'\n",
        "id = link.split('/')[-2]\n",
        "downloaded = drive.CreateFile({'id' : id})\n",
        "downloaded.GetContentFile('StockPrices.csv')\n"
      ],
      "metadata": {
        "id": "l9UNIXUsgY52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code selects only the Date, Index, and Close columns from the df DataFrame and stores them in a new DataFrame called df_close.\n",
        "\n",
        "Finally, the code calls the info() method on the df_close DataFrame to print out a summary of its data, including the data types of each column and the number of non-null values."
      ],
      "metadata": {
        "id": "YLDnS4bRhDLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "df = pd.read_csv('StockPrices.csv')\n",
        "df['Date']= pd.to_datetime(df['Date'])\n",
        "df_close = df[['Date', 'Index', 'Close']]\n",
        "df_close.info()\n"
      ],
      "metadata": {
        "id": "rX2LUqNIhDom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The overall effect of this code is to create a new DataFrame (df_close) that shows the closing prices for each index on each date, with missing data removed.\n",
        "The dropna() method is then used to remove any columns with missing values (i.e., columns with at least one NaN value), and the resulting DataFrame is printed using the head() method to display the first few rows."
      ],
      "metadata": {
        "id": "-is1V8hChq_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Closing Prices Dataframe\n",
        "df_close = df_close.pivot_table(index = 'Date', columns = 'Index', values='Close').dropna(axis=1)\n",
        "df_close.head()\n"
      ],
      "metadata": {
        "id": "3EVeTwVZhrOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#      **PCA**\n",
        "The Principal Component Analysis is a popular unsupervised learning technique for reducing the dimensionality of data. Basically, PCA is a tool for identifying the main axes of variance within a data set.\n",
        "PCA is a dimensionality reduction technique that finds new orthogonal dimensions (principal components) that explain the most variance in the data.\n"
      ],
      "metadata": {
        "id": "VB-oA2g4jiDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code compresses the original DataFrame df by pivoting and stacking its data, and then drops any columns that contain missing values. The resulting DataFrame raw_df should have a simpler structure and can be used for further analysis."
      ],
      "metadata": {
        "id": "Oyt50DoiG0ZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset we are compressing, column level 0 = Stock, column level 1 = feature\n",
        "raw_df = df.drop(columns = ['Unnamed: 0','Close']).set_index(['Date' , 'Index']).unstack(level = 1).stack(level = 0).unstack()\n",
        "raw_df = raw_df.dropna(axis = 1)\n",
        "raw_df.head()"
      ],
      "metadata": {
        "id": "kE6JUZC7ji79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code first converts a DataFrame raw_df into a Numpy array, and then prints the shape of the resulting array. This can be helpful for understanding the size and structure of the data, especially when working with large datasets or complex computations."
      ],
      "metadata": {
        "id": "6AEHQ4HhHSIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = raw_df.to_numpy()\n",
        "raw_df.shape"
      ],
      "metadata": {
        "id": "IbB0-dUcjmZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is Python code that uses the scikit-learn (sklearn) library for data preprocessing and dimensionality reduction.\n",
        "The code scales the data using MinMaxScaler, and then applies principal component analysis to reduce the dimensionality of the data to 380 principal components. The resulting transformed data is stored in PCA_df.\n",
        "\n",
        "MinMaxScaler scales the data to a specified range, usually between 0 and 1. The fit_transform() method scales the data and returns the scaled version, which is stored in the raw_df_scaled variable."
      ],
      "metadata": {
        "id": "pQDP6g0KHiKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#Scaling the data\n",
        "raw_df_scaled = MinMaxScaler().fit_transform(raw_df)\n",
        "\n",
        "#Performing PCA ~ Reducing Dimensionality\n",
        "PCA = PCA(n_components=380)\n",
        "PCA_df = PCA.fit_transform(raw_df_scaled)"
      ],
      "metadata": {
        "id": "fsPOXONajoje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, the code is creating a line plot that shows how much of the total variance in a dataset is explained by each additional principal component. This can be useful for determining how many principal components are needed to capture a desired amount of variance in the data."
      ],
      "metadata": {
        "id": "tg2xRg0XH-y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.cumsum(PCA.explained_variance_ratio_))\n",
        "plt.xlabel('Num Components')\n",
        "plt.ylabel('Cumulative Explained Variance');"
      ],
      "metadata": {
        "id": "IziMXcIwjqQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates a list of labels for principal component analysis (PCA) results.\n",
        "\n",
        "The first line of code retrieves the index values from the \"df_close\" DataFrame and assigns them to the variable \"dates\".\n",
        "\n",
        "The second line of code retrieves the column labels from the \"df_close\" DataFrame and assigns them to the variable \"stocks\".\n",
        "\n",
        "The third line of code initializes an empty list called \"PC_labs\".\n",
        "\n",
        "The fourth line of code starts a loop that will iterate over the columns of a DataFrame called \"PCA_df\" (which is not shown in this code snippet).\n",
        "\n",
        "After all iterations of the loop are complete, the \"PC_labs\" list will contain a set of labels that can be used to identify the results of a PCA analysis. For example, the label \"PC1\" would correspond to the first principal component, and so on."
      ],
      "metadata": {
        "id": "z2Rw1djqJOEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dates = df_close.index\n",
        "stocks = df_close.columns\n",
        "PC_labs = []\n",
        "for i in range(PCA_df.shape[1]):\n",
        "  lab = \"PC\" + str(i+1)\n",
        "  PC_labs.append(lab)"
      ],
      "metadata": {
        "id": "ZzQE6ULzjtAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression Prediction Functions**\n",
        "\n",
        "It uses linear relationships between a dependent variable (target) and one or more independent variables (predictors) to predict the future of the target."
      ],
      "metadata": {
        "id": "TqY7yCqh-mc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function takes in the following arguments:\n",
        "\n",
        "1. raw_df: A pandas DataFrame containing the features used for the prediction.\n",
        "2. close: DataFrame containing the closing prices of the stocks being predicted.\n",
        "3. time: An integer specifying the current time step.\n",
        "4. lookback: The number of time steps to look back when creating the input features.\n",
        "5. forward: The number of time steps to look forward when predicting the stock prices.\n",
        "5. stock_num: An integer specifying the index of the stock being predicted.\n",
        "\n",
        "The function first creates two PCA objects, pca1 and pca2, each with 10 principal components.\n",
        "\n",
        "Then, it creates the training data and testing data. The training data is created by taking the raw features from (time - forward - lookback) to (time - forward), scaling the data using MinMaxScaler, and applying PCA to reduce the dimensionality. The corresponding target values for the training data are taken from close for the same time period.\n",
        "\n",
        "The testing data is created by taking the raw features from (time - lookback) to (time), scaling the data using MinMaxScaler, and applying PCA to reduce the dimensionality. The corresponding target values for the testing data are taken from close for (time + 1) to (time + forward + 1)\n",
        "\n",
        "Next, the function creates a LinearRegression object, LR, and fits it to the training data using LR.fit(X_train, y_train).\n",
        "\n",
        "Finally, the function predicts the stock prices for the testing data using LR.predict(X_test) and returns the predicted values and the actual target values y_test."
      ],
      "metadata": {
        "id": "MLU8wtqHV_8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#Using the full features dataset, the closing prices; we are able to fit a line over a specified time period\n",
        "def predict_prices(raw_df, close, time, lookback, forward, stock_num):\n",
        "\n",
        "  #PCA\n",
        "  pca1 = PCA(n_components = 10)\n",
        "  pca2 = PCA(n_components = 10)\n",
        "\n",
        "\n",
        "  #Training data = t - forward - lookback\n",
        "  X_train = raw_df[time-forward-lookback:time-forward,:]\n",
        "  X_train = MinMaxScaler().fit_transform(X_train)\n",
        "  X_train = pca1.fit_transform(X_train)\n",
        "  y_train = close.iloc[time-forward+1:time+1,stock_num]\n",
        "\n",
        "  #Testing = t - lookback\n",
        "  X_test = raw_df[time-lookback:time,:]\n",
        "  X_test = MinMaxScaler().fit_transform(X_test)\n",
        "  X_test = pca2.fit_transform(X_test)\n",
        "  y_test = close.iloc[time+1 : time+forward+1, stock_num]\n",
        "\n",
        "  LR = LinearRegression()\n",
        "  LR.fit(X_train, y_train)\n",
        "  predicted = LR.predict(X_test)\n",
        "  # print(mean_squared_error(y_test,predicted))\n",
        "\n",
        "  return predicted, y_test"
      ],
      "metadata": {
        "id": "lMr0E47rjxYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " It calls the predict_prices function  multiple times to generate the predicted and actual values for different stocks and time periods.\n",
        "\n",
        "The function takes in two arguments:\n",
        "\n",
        "1. full_features_df: The full set of features for all stocks.\n",
        "2. closing_prices_df: The closing prices of all stocks.\n",
        "The function initializes two empty lists, predictions and actuals, which will store the predicted and actual values for each stock.\n",
        "\n",
        "The function then iterates over each stock in closing_prices_df using a for loop. For each stock, it initializes two empty lists, stock_predictions and stock_actuals.\n",
        "\n",
        "The for loop then iterates over a range of dates, starting from 60 (to allow for a lookback period of 30) and incrementing by 30 (to generate predictions for non-overlapping time periods). For each date, the function calls the predict_prices function to generate the predicted and actual values for that stock and time period.\n",
        "\n",
        "The predicted and actual values are appended to stock_predictions and stock_actuals, respectively.\n",
        "\n",
        "Finally, the function returns predictions and actuals, which together form the entire table of features for predicting stock prices.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8I24GLUMgX6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This function creates the entire table of features\n",
        "def construct_prediction_tab(full_features_df,closing_prices_df):\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "\n",
        "  for stocks in range(closing_prices_df.shape[1]):\n",
        "    stock_predictions = []\n",
        "    stock_actuals = []\n",
        "\n",
        "    for dates in range(60, df_close.shape[0], 30):\n",
        "      pred, act = predict_prices(full_features_df, closing_prices_df, dates, 30, 30, stocks)\n",
        "      stock_predictions.append(pred)\n",
        "      stock_actuals.append(act)\n",
        "\n",
        "    import numpy as np\n",
        "    stock_predictions = np.concatenate(stock_predictions)\n",
        "    stock_actuals = np.concatenate(stock_actuals)\n",
        "\n",
        "    predictions.append(stock_predictions)\n",
        "    actuals.append(stock_actuals)\n",
        "\n",
        "  return predictions, actuals"
      ],
      "metadata": {
        "id": "0NR8jDd2j2Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore, pred and act are likely to be two lists of predicted and actual values, respectively, for all stocks and time periods, which together form the entire table of features for predicting stock prices."
      ],
      "metadata": {
        "id": "TJ9M2fh_hkkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred, act = construct_prediction_tab(raw_df, df_close)"
      ],
      "metadata": {
        "id": "eOUGup-Dj4Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data argument is set to act, which is a 2D numpy array with one row per stock and one column per time period. The index argument is set to stocks. The columns argument is set to dates[61:], which is a slice of the dates variable starting from index 61 (to remove the initial prediction window of 60 days) and continuing to the end of the list. The resulting DataFrame has one row per time period and one column per stock, with the actual values of each stock's closing price at each time period.\n",
        "\n",
        "The transpose() method is called on both DataFrames to swap the rows and columns, so that the columns correspond to the stocks and the rows correspond to the time periods."
      ],
      "metadata": {
        "id": "InnH-Xjehlda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to get rid of 60 days for initial prediction window\n",
        "final_actuals = pd.DataFrame(data = act, index=stocks, columns = dates[61:]).transpose()\n",
        "final_preds = pd.DataFrame(data = pred, index = stocks).transpose()\n"
      ],
      "metadata": {
        "id": "qx2P5IN_j6Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is used to trim the final_preds DataFrame to match the time range of the final_actuals DataFrame, and also to set the index of the final_preds DataFrame to the same range of dates as the final_actuals DataFrame."
      ],
      "metadata": {
        "id": "qXTUaETFikPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_preds = final_preds.iloc[:4966,:]\n",
        "final_preds.index = dates[61:]"
      ],
      "metadata": {
        "id": "Nx6XpPxqj8Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_actuals.head()"
      ],
      "metadata": {
        "id": "JDkyVJ4Sj_FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_preds.head()"
      ],
      "metadata": {
        "id": "LByTGBQCkBRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code exports the final_actuals and final_preds DataFrames to CSV files and saves them to Google Drive using the Google Colab interface."
      ],
      "metadata": {
        "id": "PDqJ6RkSjKv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "\n",
        "final_actuals.to_csv('LR_Actual_Prices.csv')\n",
        "!cp LR_Actual_Prices.csv \"drive/My Drive/Machine Learning Project/ML Section Exports\"\n",
        "\n",
        "final_preds.to_csv('LR_Predicted_Prices.csv')\n",
        "!cp LR_Predicted_Prices.csv \"drive/My Drive/Machine Learning Project/ML Section Exports\"\n"
      ],
      "metadata": {
        "id": "sCf3aL15kDfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The start time of the prediction window, time, the number of days to look back for features, lookback, the number of days to predict in the future, forward, and the stock number to predict for, stock_num.\n",
        "\n",
        "The output of each call is two arrays: p1, p2, and p3, which contain the predicted prices for the given prediction window, and t1, t2, and t3, which contain the actual prices for the same window."
      ],
      "metadata": {
        "id": "lEEIgJwzjP3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Three different Prediction Windows\n",
        "p1 , t1 = predict_prices(raw_df, df_close, 60, 30, 30, 5)\n",
        "p2 , t2 = predict_prices(raw_df, df_close, 90, 30, 30, 5)\n",
        "p3 , t3 = predict_prices(raw_df, df_close, 120, 30, 30, 5)"
      ],
      "metadata": {
        "id": "rs090vXLkGu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predictions = np.concatenate([p1,p2,p3])\n",
        "actuals = np.concatenate((t1,t2))"
      ],
      "metadata": {
        "id": "NnF71vyTkIDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a plt for the first 90 days of predictions for the first stock\n",
        "plt.plot(predictions, label = 'predicted')\n",
        "plt.plot(actuals, label = 'Actual')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3NWJ2OrrkJlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_predictions = []\n",
        "stock_actuals = []\n",
        "\n",
        "for i in range(60,df_close.shape[0], 30):\n",
        "  pred, act = predict_prices(raw_df, df_close, i, 30, 30, 5)\n",
        "  stock_predictions.append(pred)\n",
        "  stock_actuals.append(act)\n"
      ],
      "metadata": {
        "id": "T7GWLNUckLVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_predictions = np.concatenate(stock_predictions)\n",
        "stock_actuals = np.concatenate(stock_actuals)"
      ],
      "metadata": {
        "id": "tFtIfXW3kOZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a scatter plot of the predicted stock prices (stock_predictions) on the x-axis and the actual stock prices (stock_actuals) on the y-axis.\n",
        "\n",
        "This is because the predictions were made for a longer period than the actual prices, and we only want to compare the predicted prices to the actual prices within the prediction window.\n",
        "\n",
        "The resulting scatter plot can be used to visually compare the predicted and actual prices, and to assess how well the predictions match the actual values. If the points on the scatter plot fall close to a diagonal line, it indicates a good fit between the predicted and actual values. If the points are scattered or show a systematic deviation from the diagonal line, it indicates a poor fit.\n",
        "\n",
        "The term \"Q-Q plot\" stands for \"quantile-quantile plot\", and is a type of graphical comparison of two probability distributions. In this case, it is used to compare the distribution of the predicted stock prices to the distribution of the actual stock prices.\n",
        "\n"
      ],
      "metadata": {
        "id": "VIOk8YLckixD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Q-Q plot for predictions vs actuals\n",
        "plt.scatter(x = stock_predictions[:4966], y = stock_actuals)"
      ],
      "metadata": {
        "id": "_BmsALPskPpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is creating a plot of the predicted closing stock prices versus the actual closing stock prices for a specific stock."
      ],
      "metadata": {
        "id": "mr9kVlIglAIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Full Prediction vs Actuals for the same stock\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(stock_predictions, label = 'Predicted')\n",
        "plt.plot(stock_actuals, label = 'Actual')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "S_SvrIE4kRzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of this code will be a single floating point value representing the MAE between the actual and predicted values."
      ],
      "metadata": {
        "id": "frM4_n7xlUN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "mean_absolute_error(final_actuals, final_preds)"
      ],
      "metadata": {
        "id": "0HT6scKhkTLt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}